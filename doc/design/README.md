# 这里是vscrawler的设计说明

## 序
vscrawler为何产生? 其实并不是想抢当前已经存在的爬虫框架的饭碗。不过确实是因为使用当前已经存在的爬虫框架不能解决我所面临的问题,所以才撸一套。
需要申明一点,本人并不想普渡众生,写这个框架的主要目的是使得个人使用顺手,如果你有好的想法或者issue,欢迎提交,但是个人没有义务帮你解决使用上的问题。
这样说也是因为vscrawler的定义在数据抓取而非简单爬虫,vscrawler在框架灵活性和简便性上面多会选择前者,导致vscrawler的使用难度原因高于webmagic,WebCollector等框架。
vscrawler不是入门级的爬虫框架,拒绝菜鸟入坑
以下几种情况会被认为不适合使用本爬虫:

- 我的爬虫经常报timeout的异常,什么原因啊?
- 如何提交post请求?
- html是js产生的,应该怎么爬取啊?
- 为什么我下载下来的网页看不到数据,但是浏览器能够看到?
- 如何下载图片?
- 网页需要登录,应该怎么办?
- 抓取遇到验证码,应该怎么办?
- 求问大佬，爬网页的时候突然开始抛Socket超时的异常，这是啥情况，是IP被封了吗？用HtmlUnit的getPage打开百度就正常， 打开我要爬得网页就抛Socket超时异常
- 这个title 我该怎么获取?
- xpath规则应该怎么写

上面的问题,应该是你能够自己想到答案的,而不需要像他人请教,或者问题太宽泛,其他人也不能回答。这样的表现使用vscrawler应该是很困难的。


## 对比和吐槽

有人喜欢造轮子,原因我总结如下
- 为了学习,模仿成熟框架实现一个demo,多在于学生时代,找工作的时候面试官很喜欢看到这种
- 为了装逼
- 市面上产品不好用,为了满足自己的需求会把框架使用得比较恶心
- 对市面上产品不了解,别人有实现完善的自己不知道
- 市面上有已经实现了的,但是个人比较高冷,我自己实现一个,用得更加顺手

写vscrawler的动机是啥呢?嗯,装逼


## 作者简介

为啥装逼,是因为我太年轻。上次写了一个叫做dungproxy的代理层中间件,有同学突然发现使用的是一个刚刚毕业不到半年的本科生空闲时间写的框架,很是担忧。觉得入坑很危险。
所以这里先声明,本人现在毕业不到一年,年轻人会比较高调,如果你觉得vscrawler的设计不靠谱,也许你的猜想是对的呢。

作者介绍:2016年毕业于四川大学软件学院,长期作为一个没有工程能力的大神。喜欢写工具。擅长c语言。但是好多年不写c。目前主要使用java作为编程语言。当前已经开源的项目列表如下:

|项目|简介|
|---|---|---|
|[spring,springMVC,mybatis代码生成器](http://git.oschina.net/virjar/ssm-gen)|大学毕业设计,学校说毕业设计版权是学校的,但是学校只是把它放到资料库里面了,所以今后还是小改一下变成我个人的吧。|
|[dungproxy](http://git.oschina.net/virjar/proxyipcenter)|代理IP中间层,发源于大学三年级在企业实习时老大的项目要求,经过打磨,已经成为部分企业用在生产环境的框架了,其server端基于ssm-gen开发|
|[vscrawler](http://git.oschina.net/virjar/vscrawler)| 抓取框架,内置各种爬虫封堵策略,便于灵活的获取目标网站数据,目前不可用在生产环境,其中网络层使用dungproxy|
|[jscrack](http://git.oschina.net/virjar/jscrack)|js注入工具,将自定义js文件注入到目标网站,用于破解网站加解密协议|
|[jsrepair](http://git.oschina.net/virjar/jsrepair)|js反混淆工具,对进行过混淆压缩的js代码进行美化,主要体现在预计算,格式化,逗号表达式拆卸,三目运算符拆卸等功能|


## 继续对比
webmagic,我了解webmagic是很多人用在了生产环境上,webmagic本身的代码结构是非常优秀的,架构简洁清晰,功能扩展比较方便。xsoup抽取器很强大和方便。

webCollector: 看packagename,应该也是一位同学大学开始写的代码吧。合肥工业大学? webCollector我觉得使用伯克利DB挺好,适合管理大量任务,适合断点续爬。但是使用jdk内置的URLConnection访问网络,以及代码结构问题,我感觉功能扩展很不方便。
然后就是方法写太长了,很喜欢用抽象类,但是java单继承特性导致功能扩展很不方便

SpiderMan: webmagic作者前期对webMagic的介绍里面,有说借鉴了SpiderMan。说明SpiderMan是一个比较老的爬虫框架吧。SpiderMan本身很有特点,就是基于配置文件的爬虫。这也是很多人希望做到的,但是似乎做到这点儿很难。方便性和灵活性的均衡很不好做,spiderMan的配置文件肯定
导致灵活性降低,如果强行实现xml文件不容易描述的规则,那么导致SpiderMan的语法将会特别复杂。所以大多数人应该没有学习xml语法的欲望吧。因为这套语法不是之前就有的规范,而是spiderMan自己定义。不过spiderman的研究者应该不少,
很多公司所谓的leader会给手下程序员安排这种类型的活儿。leader觉得爬虫通过配置来实现最好,然后让底下的人实现,底下的人百度了一下就能找到SpiderMan。另一个评价,spiderMan只支持1.8,喜欢函数式,喜欢定义大量内部类,代码结构不太清晰。要知道国内大多数企业应该还是1.7的标准。估计不少同学会入坑

gatherplatform:没有仔细看,作者qq号码很牛逼,群里很少发言,发言内容只有打广告,没有看到过讨论技术或者回答问题。看介绍也是cms的爬虫吧。爬爬文章,新闻还可以。我的意思,如果使用这个,可以考虑一个叫做神箭手的爬虫平台

seimicrawler: 这个我很佩服,可惜出身太晚了,因为他的出现套路和webmagic几乎一致(受python的Scrapy启发,因为xpath不方便把jsoup单独抽取出现实现xpath和cssQuery的结合),这是一个大神,为了解决动态网页问题,能够自己封装一个浏览器。话说其他的孩子都只能通过jdk调本地浏览器。而他这个去掉了java层,直接跨语言跨机器通信
我个人理解是所有基于浏览器方案里面最稳定的一种。而且他的jsoupXpath比webmagic的xsoup更加完善。将是vscrawler后续架构的重点学习项目

